{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtO9jMGEaShb"
      },
      "source": [
        "# Blind source separation\n",
        "The purpose of the project is to separate an image obtained as a sum of a two images into its components. \n",
        "\n",
        "The two images img1 and img2 summed together come from different dataset: mnist and fashion_mnist, respectively.\n",
        "\n",
        "No preprocessing is allowed. The network takes in input the sum img1+img2 and returns the predicted components hat_img1 and hat_img2. \n",
        "\n",
        "The metric used to evaluate the project is the mean squared error between predicted and ground truth images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wrlTnZCNfnfr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRotM-TfcUcn"
      },
      "source": [
        "Here we load the two datasets, mnist and fashion mnist (both in grayscale).\n",
        "\n",
        "For simplicity, the samples are padded to dimension (32,32)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRYiW2ipukZF",
        "outputId": "0b15757c-f27a-4012-a011-f82997742de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 32, 32)\n"
          ]
        }
      ],
      "source": [
        "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
        "print(np.shape(mnist_x_train))\n",
        "(fashion_mnist_x_train, fashion_mnist_y_train), (fashion_mnist_x_test, fashion_mnist_y_test) = fashion_mnist.load_data()\n",
        "#normnalize in and pad\n",
        "mnist_x_train = np.pad(mnist_x_train,((0,0),(2,2),(2,2)))/255.\n",
        "print(np.shape(mnist_x_train))\n",
        "mnist_x_test = np.pad(mnist_x_test,((0,0),(2,2),(2,2)))/255.\n",
        "fashion_mnist_x_train = np.pad(fashion_mnist_x_train,((0,0),(2,2),(2,2)))/255.\n",
        "fashion_mnist_x_test = np.pad(fashion_mnist_x_test,((0,0),(2,2),(2,2)))/255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBIWB2a-VGqH",
        "outputId": "9c2af9f3-3974-4062-c4bd-eea78e321d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 32, 32)\n"
          ]
        }
      ],
      "source": [
        "print(np.shape(mnist_x_train))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or5X_DyPc6dT"
      },
      "source": [
        "Here is a simple datagenerator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Y5Zpv5fw2hd"
      },
      "outputs": [],
      "source": [
        "def datagenerator(x1,x2,batchsize):\n",
        "    n1 = x1.shape[0]\n",
        "    n2 = x2.shape[0]\n",
        "    while True:\n",
        "        num1 = np.random.randint(0, n1, batchsize)\n",
        "        num2 = np.random.randint(0, n2, batchsize)\n",
        "\n",
        "        x_data = (x1[num1] + x2[num2]) / 2.0\n",
        "        y_data = np.concatenate((x1[num1], x2[num2]), axis=2)\n",
        "\n",
        "        yield x_data, y_data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JLwLpsrdVH-"
      },
      "source": [
        "Here we define two generators, one for training and one for testing. You may possibly add an addition generator for validation, further splitting the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xc95qrN72Ysu"
      },
      "outputs": [],
      "source": [
        "batchsize = 64\n",
        "inputShape = (32,32,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dkv8Qc9-wEnf"
      },
      "outputs": [],
      "source": [
        "train_generator = datagenerator(mnist_x_train,fashion_mnist_x_train,80000)\n",
        "val_generator = datagenerator(mnist_x_train,fashion_mnist_x_train,40000)\n",
        "test_generator = datagenerator(mnist_x_test,fashion_mnist_x_test,20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FqBi8C3uOEOr"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = next(train_generator)\n",
        "\n",
        "x_val, y_val = next(val_generator)\n",
        "x_test, y_test = next(train_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiVcyHrVeXpI"
      },
      "source": [
        "Let us look at some input-output pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "MPslGyNdxUQS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "fe761b76-e6a4-42dd-f485-6120ff94b621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32)\n",
            "(80000, 32, 64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAToUlEQVR4nO3db6xV5ZXH8e8SQRFQQIUitVxxoEAopQ2lKsZ22thg24S2mdjaTOMLU5qJJjbpvDBOMnXmVTuZtmnfdEpHop10bJ3+pWkzg1oVfQMFxgoVFLBQQQRREBDkn2tenE1yYc5a9959/t3r8/skhHOfdfc5ix3W3efudZ7nMXdHRN75Luh1AiLSHSp2kUKo2EUKoWIXKYSKXaQQKnaRQlzYysFmthT4LjAK+Hd3/8YA368+n0iHubs1G7e6fXYzGwW8ANwM7Ab+ANzm7s8lx6jYRTosKvZW3sYvBra7+4vufhL4CbCshecTkQ5qpdinAy/1+3p3NSYiw1BLv7MPhpktB5Z3+nVEJNdKse8Bru739bursXO4+wpgBeh3dpFeauVt/B+AWWZ2jZmNAb4ArGpPWiLSbrWv7O5+2szuAv6HRuttpbv/qW2ZiUhb1W691XoxvY0X6bhOtN5EZARRsYsUQsUuUggVu0ghVOwihej4J+hKYNb05icA3V7Qc8mSJU3HL7300vCYbdu21XqtmTNnDvmY1atX13otaZ2u7CKFULGLFELFLlIIFbtIIVTsIoUo8rPxw+Xu+bhx48LY4sWLw9h73/veMLZp06am41dddVV4zKRJk8LYqFGjwtjBgwfD2N69e4ecx44dO8LYunXrwpicS5+NFymcil2kECp2kUKo2EUKoWIXKYSKXaQQRbbe2m3+/PlhbNasWWFs+vR4mf1Tp06FsaxF9frrrzcdf+2118JjbrjhhjCWtd6eeuqpMDZx4sSm41OmTAmPmTFjRq08du/eHcaefPLJpuNHjx4Njxnp1HoTKZyKXaQQKnaRQqjYRQqhYhcphIpdpBAttd7MbCdwBDgDnHb3RQN8/4huvX3qU59qOp61106cOBHGsvZPNsvr9OnTYSyawZa1rrIcL7ggvh6MHTs2jJ05c6bpePZvPnLkSBj78Ic/HMaymXRvvfVW0/EnnngiPGbr1q1hbCSIWm/tWHDyr939QBueR0Q6SG/jRQrRarE7sNrMNpjZ8nYkJCKd0erb+BvdfY+ZTQEeMbOt7r6m/zdUPwT0g0Ckx1q6srv7nurv/cAvgf+3lpK7r3D3RQPdvBORzqpd7GY2zswmnH0MfALY3K7ERKS9WnkbPxX4ZbV444XAf7r7f7clqx7KWk19fX1Nx998883wmLfffjuMZbPGLr744jA2YcKEMBbNlotaUAO9Vub48eNhLDqP2SKbY8aMCWNr164NYx/5yEfCWPR6119/fXjMSG+9RWoXu7u/CLy/jbmISAep9SZSCBW7SCFU7CKFULGLFELFLlKIdkyEeUeZO3duGIv2iLvwwvg0Pvfcc2EsmhkGeTssa+dFLa+spZgtbpnNisyeM5LN2Mtk+/NlC3AuWLCg6XjWArz88svDWLZw53CnK7tIIVTsIoVQsYsUQsUuUggVu0ghdDf+PLNnzw5j0d3zbJLJpk2bwli2FVJ21zq7C15nTcHsTncWy2Qdgzouu+yyMLZt27YwNnPmzKbj2bm/5pprwpjuxovIsKdiFymEil2kECp2kUKo2EUKoWIXKYRab+eZPn16GIu2SRo9enR4TLY1UbYV0rJly8LYnDlzwti73vWupuNZu27NmjVhbP369WEs2zbq2LFjYSyStfmyyUY333xzGIvWtctam1OnTg1jI5mu7CKFULGLFELFLlIIFbtIIVTsIoVQsYsUYsDWm5mtBD4N7Hf3+dXYZOCnQB+wE7jV3Q92Ls3uydZ+i7Z5yo553/veF8ZmzJgRxhYtivfBPHnyZBiLZoC95z3vCY9ZsmRJrdjhw4fD2AMPPNB0/NVXXw2PyVqYn//858NY1G4E2LhxY9PxbOuqbA26kWwwV/YHgKXnjd0DPObus4DHqq9FZBgbsNir/dZfP294GfBg9fhB4DNtzktE2qzu7+xT3X1v9fgVGju6isgw1vLHZd3dzSxcHsXMlgPLW30dEWlN3Sv7PjObBlD9vT/6Rndf4e6L3D2+4yQiHVe32FcBt1ePbwd+3Z50RKRTBtN6ewj4KHCFme0Gvg58A3jYzO4AdgG3djLJdssWL8xmV0WyxRVvueWWMDZq1KgwduDAgTD26KOPhrGtW7c2Hc/aSTfeeGMYy8yfPz+MffGLX2w6/r3vfS885qabbgpjV155ZRjLFojcsGFD0/Fs1tsll1wSxkayAf9nu/ttQejjbc5FRDpIn6ATKYSKXaQQKnaRQqjYRQqhYhcpRJELTk6aNCmMZXulRbGsVZO1tZ5++ukwVqe9lsn2KPvVr34VxrLz8dvf/nbIeWQttA996ENhLFuc86mnngpj0f58Wbs0WqRypNOVXaQQKnaRQqjYRQqhYhcphIpdpBAqdpFCFNl6Gz9+fBir03r73Oc+Fx6T7Yf2m9/8Joxt3759yHkMFGu3uXPnhrF58+Y1Hc/2qctaXk8++WQYmzx5chirI1v4ciTTlV2kECp2kUKo2EUKoWIXKYSKXaQQRd6Nz9aZy+5mR3d9Fy9eHB6zatWqMLZly5Ywlm0pla2fFuWfTfy46KKLwljWacjurEfn+IIL4utLdu6jCS2Qbyl16aWXhrFIlmO2bmCW43CgK7tIIVTsIoVQsYsUQsUuUggVu0ghVOwihRjM9k8rgU8D+919fjV2H/Bl4GzP4153/12nkmy3rPWWtahmz5495Od75ZVXwljWxslkbbloa6sod4AbbrghjI0dOzaM7dmzJ4xF69rdeeed4THZBJRoGyeAvr6+MDZx4sSm41mbz8zCWNamPHbsWBgbDgZzZX8AWNpk/DvuvrD6M2IKXaRUAxa7u68BXu9CLiLSQa38zn6XmT1rZivNLF6bWUSGhbrF/n3gWmAhsBf4VvSNZrbczNab2fqaryUibVCr2N19n7ufcfe3gR8C4YfD3X2Fuy9y90V1kxSR1tUqdjOb1u/LzwKb25OOiHTKYFpvDwEfBa4ws93A14GPmtlCwIGdwFc6mGPbZe2TbOZStM1TNksqW+/u7rvvDmNZW2vBggVhLGodZi3FXbt2hbFsa6UdO3aEsehcZW2tTHZctrXVrFmzar1eZCSvTzdgsbv7bU2G7+9ALiLSQfoEnUghVOwihVCxixRCxS5SCBW7SCGKXHAya59ks6GeeOKJpuPTpk1rOg6wf//+MBZtkQRw1VVXhbGsVRZtG7V169bwmD//+c9hrO7ssEg2Q/Ctt96q9VpZ/kuXNpvDBYcOHar1WlnbdrjTlV2kECp2kUKo2EUKoWIXKYSKXaQQKnaRQhTZesvaSVks8vvf/z6M/eAHPwhj1113XRg7depUGMvaRsePHw9jdWRtqDoLZp48eTKMvfjii2Esa9m9/PLLYSw7j5Hs/8CYMWOG/HzDha7sIoVQsYsUQsUuUggVu0ghVOwihSjybny2zlwmuvt8+vTp8JiDBw+GsTfffLNWLLvDHN09z3LM7nRnd6az8xhtUZWt13f06NEwFq1pN5ATJ040Hc/W5MtidbfsGg50ZRcphIpdpBAqdpFCqNhFCqFiFymEil2kEIPZ/ulq4EfAVBrbPa1w9++a2WTgp0AfjS2gbnX3uM80jLR7C5+srZXJ1jM7fPhwGMsmY0S51M2x7nZNCxcubDqetd42btwYxrJ2WCZaAzD7P/DGG2+EsZG8/dNgruynga+5+zzgOuBOM5sH3AM85u6zgMeqr0VkmBqw2N19r7tvrB4fAbYA04FlwIPVtz0IfKZTSYpI64b0O7uZ9QEfANYCU919bxV6hcbbfBEZpgb9cVkzGw/8HPiqux/u/7ucu7uZNf1cpZktB5a3mqiItGZQV3YzG02j0H/s7r+ohveZ2bQqPg1oeifE3Ve4+yJ3X9SOhEWkngGL3RqX8PuBLe7+7X6hVcDt1ePbgV+3Pz0RaZfBvI1fAnwJ2GRmz1Rj9wLfAB42szuAXcCtnUmx/bKZS9lMrui41157rVYeWVsrazVls9TqtqgiWasse61sS6xIJ9paO3fubDo+Z86c8JhsnbzsfAx3Axa7uz8NRP8rP97edESkU0bujykRGRIVu0ghVOwihVCxixRCxS5SiCIXnKzb8oraLtm2RZlubjOUPV/ddlJ23LXXXtt0PDv3WSxawHIg+/btazqetd6yPEZy623kZi4iQ6JiFymEil2kECp2kUKo2EUKoWIXKYRab+fJZr1Fs7Ki/cQGUrf1VmdmW92WUd094ursiZYtwHno0KEhPx/ASy+91HQ8O/d1F9kc7nRlFymEil2kECp2kUKo2EUKoWIXKUSRd+Mz2V3ayLFjx2q91pEjR8LYJZdcEsaOHz8exurcqc/unGfPlx0X3f3Pzm822SU7V5nouKzLkHUuNBFGRIY9FbtIIVTsIoVQsYsUQsUuUggVu0ghBmy9mdnVwI9obMnswAp3/66Z3Qd8GXi1+tZ73f13nUp0OHv55ZdrHZdNoMlab5lsMkkkm/xTdy28qGWXtdeytla2JVMd2fk9depUGBvJrbfB9NlPA19z941mNgHYYGaPVLHvuPu/di49EWmXwez1thfYWz0+YmZbgOmdTkxE2mtI70nMrA/4ALC2GrrLzJ41s5VmNqnNuYlIGw262M1sPPBz4Kvufhj4PnAtsJDGlf9bwXHLzWy9ma1vQ74iUtOgit3MRtMo9B+7+y8A3H2fu59x97eBHwKLmx3r7ivcfZG7L2pX0iIydAMWuzXW6Lkf2OLu3+43Pq3ft30W2Nz+9ESkXQZzN34J8CVgk5k9U43dC9xmZgtptON2Al/pSIYd8MYbb4SxbF21qJ1Ud320uXPnhrG//OUvtZ4zaqNl/66668xlM+J27NjRdHz27NnhMVkrsq+vL4xt3jz068zhw4fD2Lhx42rFhrvB3I1/Gmi2Al+RPXWRkWrkfkJARIZExS5SCBW7SCFU7CKFULGLFKLIBSePHj1a67hsNlQdGzduDGPz5s0LY9nii9Gssqy9ls16y2abZTPAonOVzTa77LLLwlh2rurI/s3Z9k/ZrL3hTld2kUKo2EUKoWIXKYSKXaQQKnaRQqjYRQpRZOsta7vUXRCxjueffz6MvfDCC2FsypQpYSyalTV27NjwmCyWteyyWW+rV69uOp7Nolu5cmUYa/eCk9m/K5vZVnch0OFAV3aRQqjYRQqhYhcphIpdpBAqdpFCqNhFClFk623Xrl1h7MCBA2Gs7sKSdbh7GNu3b1/X8mi3xx9/vNcpALBu3bowlrUUs+OGO13ZRQqhYhcphIpdpBAqdpFCqNhFCmHZXV8AM7sYWANcROPu/c/c/etmdg3wE+ByYAPwJXdPZyuYWf5iItIyd2+6iN5gruwngI+5+/tpbM+81MyuA74JfMfd/wo4CNzRrmRFpP0GLHZvOLsc6+jqjwMfA35WjT8IfKYjGYpIWwx2f/ZR1Q6u+4FHgB3AIXc/Oyl4NzC9MymKSDsMqtjd/Yy7LwTeDSwG5gz2BcxsuZmtN7P1NXMUkTYY0t14dz8EPA5cD0w0s7Mft303sCc4ZoW7L3L3RS1lKiItGbDYzexKM5tYPR4L3AxsoVH0f1N92+3ArzuVpIi0bjCttwU0bsCNovHD4WF3/2czm0mj9TYZ+F/gb939xADPpdabSIdFrbcBi72dVOwinddKn11E3gFU7CKFULGLFELFLlIIFbtIIbq9Bt0B4OwCcFdUX/ea8jiX8jjXSMtjRhToauvtnBc2Wz8cPlWnPJRHKXnobbxIIVTsIoXoZbGv6OFr96c8zqU8zvWOyaNnv7OLSHfpbbxIIXpS7Ga21MyeN7PtZnZPL3Ko8thpZpvM7JluLq5hZivNbL+Zbe43NtnMHjGzbdXfk3qUx31mtqc6J8+Y2Se7kMfVZva4mT1nZn8ys7ur8a6ekySPrp4TM7vYzNaZ2R+rPP6pGr/GzNZWdfNTMxszpCd2967+oTFVdgcwExgD/BGY1+08qlx2Alf04HVvAj4IbO439i/APdXje4Bv9iiP+4C/7/L5mAZ8sHo8AXgBmNftc5Lk0dVzAhgwvno8GlgLXAc8DHyhGv834O+G8ry9uLIvBra7+4veWHr6J8CyHuTRM+6+Bnj9vOFlNNYNgC4t4Bnk0XXuvtfdN1aPj9BYHGU6XT4nSR5d5Q1tX+S1F8U+HXip39e9XKzSgdVmtsHMlvcoh7Omuvve6vErwNQe5nKXmT1bvc3v+K8T/ZlZH/ABGleznp2T8/KALp+TTizyWvoNuhvd/YPALcCdZnZTrxOCxk92Gj+IeuH7wLU09gjYC3yrWy9sZuOBnwNfdffD/WPdPCdN8uj6OfEWFnmN9KLY9wBX9/s6XKyy09x9T/X3fuCXNE5qr+wzs2kA1d/7e5GEu++r/qO9DfyQLp0TMxtNo8B+7O6/qIa7fk6a5dGrc1K99pAXeY30otj/AMyq7iyOAb4ArOp2EmY2zswmnH0MfALYnB/VUatoLNwJPVzA82xxVT5LF86JmRlwP7DF3b/dL9TVcxLl0e1z0rFFXrt1h/G8u42fpHGncwfwDz3KYSaNTsAfgT91Mw/gIRpvB0/R+N3rDhp75j0GbAMeBSb3KI//ADYBz9IotmldyONGGm/RnwWeqf58stvnJMmjq+cEWEBjEddnafxg+cd+/2fXAduB/wIuGsrz6hN0IoUo/QadSDFU7CKFULGLFELFLlIIFbtIIVTsIoVQsYsUQsUuUoj/AzME4Xv/2qZCAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print(x_train[0].shape)\n",
        "print(y_train.shape)\n",
        "plt.imshow(x_train[0],cmap='gray', interpolation='nearest')\n",
        "plt.show()\n",
        "plt.imshow(y_train[0], cmap='gray', interpolation='nearest')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wbcNrbDdCB_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neural network i chose to face this problem is UNET. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. This structure is one of the most preminent in segmentation tasks. The original architecture is proposed here: https://arxiv.org/abs/1505.04597\n",
        "I tried different alternatives of UNET, but the best results are the ones that i reached refining the following: https://github.com/bnsreenu/python_for_microscopists/blob/master/204-207simple_unet_model.py.\n",
        "To improve it, i decided to commenting some dropout layers, adding BatchNormalization layers before the relu activations, increase the filters size, and concatenate two output layers since for our task we need a 32x64 image."
      ],
      "metadata": {
        "id": "Oizmbd43B9ox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evzwp5OR6h4K",
        "outputId": "58ce5ae2-74af-44d8-fbc4-32a828a39806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 32)   320         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 32)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 32)  128         ['activation[0][0]']             \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 32)   9248        ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 32, 32, 32)   0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 32)  128         ['activation_1[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 16, 16, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 16, 16, 64)   18496       ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 16, 16, 64)   0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 16, 16, 64)  256         ['activation_2[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 16, 16, 64)   36928       ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 16, 16, 64)   0           ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 16, 16, 64)  256         ['activation_3[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 64)    0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 8, 8, 128)    73856       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 8, 8, 128)    0           ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 8, 8, 128)   512         ['activation_4[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 8, 8, 128)    147584      ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 8, 8, 128)    0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 8, 8, 128)   512         ['activation_5[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 128)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 4, 4, 256)    295168      ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 4, 4, 256)    0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 4, 4, 256)   1024        ['activation_6[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 4, 4, 256)    590080      ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 4, 4, 256)    0           ['conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 4, 4, 256)   1024        ['activation_7[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 2, 2, 256)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 2, 2, 512)    1180160     ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 2, 2, 512)    0           ['conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 2, 2, 512)   2048        ['activation_8[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 2, 2, 512)    2359808     ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTransp  (None, 4, 4, 256)   524544      ['conv2d_9[0][0]']               \n",
            " ose)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 4, 4, 512)    0           ['conv2d_transpose[0][0]',       \n",
            "                                                                  'batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 4, 4, 256)    1179904     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 4, 4, 256)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 4, 4, 256)   1024        ['activation_9[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 4, 4, 256)    590080      ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 4, 4, 256)   1024        ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2DTran  (None, 8, 8, 128)   131200      ['batch_normalization_10[0][0]'] \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 8, 8, 256)    0           ['conv2d_transpose_1[0][0]',     \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 8, 8, 128)    295040      ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 8, 8, 128)    0           ['conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 8, 8, 128)   512         ['activation_11[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 8, 8, 128)    147584      ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 8, 8, 128)    0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 8, 8, 128)   512         ['activation_12[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2DTran  (None, 16, 16, 64)  32832       ['batch_normalization_12[0][0]'] \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 16, 16, 128)  0           ['conv2d_transpose_2[0][0]',     \n",
            "                                                                  'batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 64)   73792       ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16, 64)   0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 64)  256         ['activation_13[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 16, 16, 64)   36928       ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 16, 16, 64)   0           ['conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 16, 64)  256         ['activation_14[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2DTran  (None, 32, 32, 32)  8224        ['batch_normalization_14[0][0]'] \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 32, 32, 64)   0           ['conv2d_transpose_3[0][0]',     \n",
            "                                                                  'batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 32, 32, 32)   18464       ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 32, 32, 32)   0           ['conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 32, 32, 32)  128         ['activation_15[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 32, 32, 32)   9248        ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 32, 32, 32)   0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 32, 32, 32)  128         ['activation_16[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 32, 32, 1)    33          ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 32, 32, 1)    33          ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 32, 64, 1)    0           ['conv2d_18[0][0]',              \n",
            "                                                                  'conv2d_19[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,769,282\n",
            "Trainable params: 7,764,418\n",
            "Non-trainable params: 4,864\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from keras.models import Model\n",
        "inputs = tf.keras.layers.Input(inputShape)\n",
        "\n",
        "c1 = tf.keras.layers.Conv2D(32, (3, 3), padding='same')(inputs)\n",
        "\n",
        "c1 = tf.keras.layers.Activation(\"relu\")(c1)\n",
        "c1 = tf.keras.layers.BatchNormalization()(c1)\n",
        "#c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "c1 = tf.keras.layers.Conv2D(32, (3, 3),  padding='same')(c1)\n",
        "\n",
        "c1 = tf.keras.layers.Activation(\"relu\")(c1)\n",
        "c1 = tf.keras.layers.BatchNormalization()(c1)\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "c2 = tf.keras.layers.Conv2D(64, (3, 3),  padding='same')(p1)\n",
        "\n",
        "c2 = tf.keras.layers.Activation(\"relu\")(c2)\n",
        "c2 = tf.keras.layers.BatchNormalization()(c2)\n",
        "\n",
        "#c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "c2 = tf.keras.layers.Conv2D(64, (3, 3),  padding='same')(c2)\n",
        "\n",
        "c2 = tf.keras.layers.Activation(\"relu\")(c2)\n",
        "c2 = tf.keras.layers.BatchNormalization()(c2)\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "c3 = tf.keras.layers.Conv2D(128, (3, 3),  padding='same')(p2)\n",
        "\n",
        "c3 = tf.keras.layers.Activation(\"relu\")(c3)\n",
        "c3 = tf.keras.layers.BatchNormalization()(c3)\n",
        "#c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
        "c3 = tf.keras.layers.Conv2D(128, (3, 3),  padding='same')(c3)\n",
        "\n",
        "c3 = tf.keras.layers.Activation(\"relu\")(c3)\n",
        "c3 = tf.keras.layers.BatchNormalization()(c3)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "c4 = tf.keras.layers.Conv2D(256, (3, 3),  padding='same')(p3)\n",
        "c4 = tf.keras.layers.Activation(\"relu\")(c4)\n",
        "c4 = tf.keras.layers.BatchNormalization()(c4)\n",
        "\n",
        "#c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
        "c4 = tf.keras.layers.Conv2D(256, (3, 3),  padding='same')(c4)\n",
        "\n",
        "c4 = tf.keras.layers.Activation(\"relu\")(c4)\n",
        "c4 = tf.keras.layers.BatchNormalization()(c4)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "c5 = tf.keras.layers.Conv2D(512, (3, 3),  padding='same')(p4)\n",
        "\n",
        "c5 = tf.keras.layers.Activation(\"relu\")(c5)\n",
        "c5 = tf.keras.layers.BatchNormalization()(c5)\n",
        "\n",
        "#c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
        "c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu',  padding='same')(c5)\n",
        "\n",
        "#Expansive path \n",
        "u6 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "c6 = tf.keras.layers.Conv2D(256, (3, 3),  padding='same')(u6)\n",
        "\n",
        "c6 = tf.keras.layers.Activation(\"relu\")(c6)\n",
        "c6 = tf.keras.layers.BatchNormalization()(c6)\n",
        "#c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
        "c6 = tf.keras.layers.Conv2D(256, (3, 3),  padding='same')(c6)\n",
        "c7 = tf.keras.layers.Activation(\"relu\")(c6)\n",
        "c6 = tf.keras.layers.BatchNormalization()(c6)\n",
        "\n",
        "\n",
        "u7 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u7)\n",
        "\n",
        "c7 = tf.keras.layers.Activation(\"relu\")(c7)\n",
        "c7 = tf.keras.layers.BatchNormalization()(c7)\n",
        "#c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
        "c7 = tf.keras.layers.Conv2D(128, (3, 3),  padding='same')(c7)\n",
        "\n",
        "c7 = tf.keras.layers.Activation(\"relu\")(c7)\n",
        "c7 = tf.keras.layers.BatchNormalization()(c7)\n",
        "\n",
        "u8 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "c8 = tf.keras.layers.Conv2D(64, (3, 3),   padding='same')(u8)\n",
        "\n",
        "c8 = tf.keras.layers.Activation(\"relu\")(c8)\n",
        "c8 = tf.keras.layers.BatchNormalization()(c8)\n",
        "#c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
        "c8 = tf.keras.layers.Conv2D(64, (3, 3),  padding='same')(c8)\n",
        "\n",
        "c8 = tf.keras.layers.Activation(\"relu\")(c8)\n",
        "c8 = tf.keras.layers.BatchNormalization()(c8)\n",
        "\n",
        "u9 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
        "c9 = tf.keras.layers.Conv2D(32, (3, 3),  padding='same')(u9)\n",
        "\n",
        "c9 = tf.keras.layers.Activation(\"relu\")(c9)\n",
        "c9 = tf.keras.layers.BatchNormalization()(c9)\n",
        "#c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
        "c9 = tf.keras.layers.Conv2D(32, (3, 3), padding='same')(c9)\n",
        "\n",
        "c9 = tf.keras.layers.Activation(\"relu\")(c9)\n",
        "c9 = tf.keras.layers.BatchNormalization()(c9)\n",
        "\n",
        "y_1 = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "y_2 = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "output = layers.Concatenate(axis=2)([y_1,y_2])\n",
        "\n",
        "model = Model(inputs=[inputs], outputs=[output])\n",
        "model.compile(tf.keras.optimizers.Adam(learning_rate=1e-3), \n",
        "              loss=tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"),\n",
        "              metrics=['mse'])\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52rIx7sQ-2N"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i fit the model. "
      ],
      "metadata": {
        "id": "LcmBUnHa_m_F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuQQ-Fpmb-0_",
        "outputId": "292e0c43-e3cb-43bb-9609-a3e9f9716c05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.6561e-04\n",
            "Epoch 1: loss improved from -inf to 0.11493, saving model to /content/gdrive/My Drive/my_model\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/my_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/my_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1250/1250 [==============================] - 43s 34ms/step - loss: 0.1149 - mse: 1.6560e-04 - val_loss: 0.1169 - val_mse: 6.2459e-04\n",
            "Epoch 2/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.6511e-04\n",
            "Epoch 2: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.6510e-04 - val_loss: 0.1169 - val_mse: 6.3490e-04\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1149 - mse: 1.6429e-04\n",
            "Epoch 3: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.6429e-04 - val_loss: 0.1168 - val_mse: 6.1525e-04\n",
            "Epoch 4/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.6238e-04\n",
            "Epoch 4: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.6238e-04 - val_loss: 0.1168 - val_mse: 6.1955e-04\n",
            "Epoch 5/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.6186e-04\n",
            "Epoch 5: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.1149 - mse: 1.6186e-04 - val_loss: 0.1168 - val_mse: 6.1410e-04\n",
            "Epoch 6/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.6266e-04\n",
            "Epoch 6: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.6267e-04 - val_loss: 0.1169 - val_mse: 6.1577e-04\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1149 - mse: 1.6074e-04\n",
            "Epoch 7: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.6074e-04 - val_loss: 0.1169 - val_mse: 6.2140e-04\n",
            "Epoch 8/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.5774e-04\n",
            "Epoch 8: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.5775e-04 - val_loss: 0.1169 - val_mse: 6.2431e-04\n",
            "Epoch 9/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.5705e-04\n",
            "Epoch 9: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.5706e-04 - val_loss: 0.1169 - val_mse: 6.1660e-04\n",
            "Epoch 10/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.5785e-04\n",
            "Epoch 10: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.5784e-04 - val_loss: 0.1169 - val_mse: 6.2293e-04\n",
            "Epoch 11/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.5741e-04\n",
            "Epoch 11: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.5740e-04 - val_loss: 0.1169 - val_mse: 6.1951e-04\n",
            "Epoch 12/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.5629e-04\n",
            "Epoch 12: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.1149 - mse: 1.5630e-04 - val_loss: 0.1169 - val_mse: 6.4403e-04\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1149 - mse: 1.5339e-04\n",
            "Epoch 13: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 29ms/step - loss: 0.1149 - mse: 1.5339e-04 - val_loss: 0.1169 - val_mse: 6.1796e-04\n",
            "Epoch 14/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.5301e-04\n",
            "Epoch 14: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.1149 - mse: 1.5303e-04 - val_loss: 0.1169 - val_mse: 6.3123e-04\n",
            "Epoch 15/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.5311e-04\n",
            "Epoch 15: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.5312e-04 - val_loss: 0.1169 - val_mse: 6.2484e-04\n",
            "Epoch 16/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1149 - mse: 1.5266e-04\n",
            "Epoch 16: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.5266e-04 - val_loss: 0.1169 - val_mse: 6.2508e-04\n",
            "Epoch 17/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1149 - mse: 1.5100e-04\n",
            "Epoch 17: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.5100e-04 - val_loss: 0.1169 - val_mse: 6.3047e-04\n",
            "Epoch 18/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1149 - mse: 1.5138e-04\n",
            "Epoch 18: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.5138e-04 - val_loss: 0.1169 - val_mse: 6.3185e-04\n",
            "Epoch 19/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.4971e-04\n",
            "Epoch 19: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.4970e-04 - val_loss: 0.1169 - val_mse: 6.2488e-04\n",
            "Epoch 20/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.4849e-04\n",
            "Epoch 20: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.4852e-04 - val_loss: 0.1169 - val_mse: 6.3929e-04\n",
            "Epoch 21/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.4847e-04\n",
            "Epoch 21: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.1149 - mse: 1.4846e-04 - val_loss: 0.1169 - val_mse: 6.3555e-04\n",
            "Epoch 22/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.4754e-04\n",
            "Epoch 22: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.4757e-04 - val_loss: 0.1169 - val_mse: 6.2964e-04\n",
            "Epoch 23/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.4736e-04\n",
            "Epoch 23: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1149 - mse: 1.4736e-04 - val_loss: 0.1169 - val_mse: 6.2767e-04\n",
            "Epoch 24/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1148 - mse: 1.4624e-04\n",
            "Epoch 24: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.4624e-04 - val_loss: 0.1169 - val_mse: 6.3700e-04\n",
            "Epoch 25/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1149 - mse: 1.4556e-04\n",
            "Epoch 25: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.4555e-04 - val_loss: 0.1170 - val_mse: 6.4145e-04\n",
            "Epoch 26/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1148 - mse: 1.4422e-04\n",
            "Epoch 26: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.4422e-04 - val_loss: 0.1169 - val_mse: 6.3421e-04\n",
            "Epoch 27/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1148 - mse: 1.4475e-04\n",
            "Epoch 27: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.4475e-04 - val_loss: 0.1169 - val_mse: 6.3536e-04\n",
            "Epoch 28/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.4343e-04\n",
            "Epoch 28: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.4343e-04 - val_loss: 0.1170 - val_mse: 6.4412e-04\n",
            "Epoch 29/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.4292e-04\n",
            "Epoch 29: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.1148 - mse: 1.4294e-04 - val_loss: 0.1170 - val_mse: 6.3169e-04\n",
            "Epoch 30/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.4219e-04\n",
            "Epoch 30: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.1148 - mse: 1.4220e-04 - val_loss: 0.1169 - val_mse: 6.3211e-04\n",
            "Epoch 31/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1148 - mse: 1.4161e-04\n",
            "Epoch 31: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.4161e-04 - val_loss: 0.1170 - val_mse: 6.3305e-04\n",
            "Epoch 32/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1148 - mse: 1.4135e-04\n",
            "Epoch 32: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.4135e-04 - val_loss: 0.1170 - val_mse: 6.5322e-04\n",
            "Epoch 33/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.4049e-04\n",
            "Epoch 33: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.4047e-04 - val_loss: 0.1169 - val_mse: 6.3400e-04\n",
            "Epoch 34/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.4068e-04\n",
            "Epoch 34: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.4067e-04 - val_loss: 0.1170 - val_mse: 6.3528e-04\n",
            "Epoch 35/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1148 - mse: 1.3872e-04\n",
            "Epoch 35: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.3872e-04 - val_loss: 0.1170 - val_mse: 6.3863e-04\n",
            "Epoch 36/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1148 - mse: 1.4051e-04\n",
            "Epoch 36: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.4051e-04 - val_loss: 0.1170 - val_mse: 6.3501e-04\n",
            "Epoch 37/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1148 - mse: 1.3814e-04\n",
            "Epoch 37: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 29ms/step - loss: 0.1148 - mse: 1.3814e-04 - val_loss: 0.1170 - val_mse: 6.3571e-04\n",
            "Epoch 38/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.3849e-04\n",
            "Epoch 38: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 29ms/step - loss: 0.1148 - mse: 1.3848e-04 - val_loss: 0.1170 - val_mse: 6.4056e-04\n",
            "Epoch 39/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.3682e-04\n",
            "Epoch 39: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.1148 - mse: 1.3683e-04 - val_loss: 0.1170 - val_mse: 6.4278e-04\n",
            "Epoch 40/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.3647e-04\n",
            "Epoch 40: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.3647e-04 - val_loss: 0.1170 - val_mse: 6.4354e-04\n",
            "Epoch 41/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.3654e-04\n",
            "Epoch 41: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.1148 - mse: 1.3653e-04 - val_loss: 0.1170 - val_mse: 6.4961e-04\n",
            "Epoch 42/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.3546e-04\n",
            "Epoch 42: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.1148 - mse: 1.3546e-04 - val_loss: 0.1170 - val_mse: 6.4870e-04\n",
            "Epoch 43/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.3581e-04\n",
            "Epoch 43: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.3581e-04 - val_loss: 0.1170 - val_mse: 6.3675e-04\n",
            "Epoch 44/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1148 - mse: 1.3381e-04\n",
            "Epoch 44: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 29ms/step - loss: 0.1148 - mse: 1.3381e-04 - val_loss: 0.1170 - val_mse: 6.4065e-04\n",
            "Epoch 45/50\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.3440e-04\n",
            "Epoch 45: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.1148 - mse: 1.3441e-04 - val_loss: 0.1170 - val_mse: 6.4764e-04\n",
            "Epoch 46/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.3384e-04\n",
            "Epoch 46: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.3383e-04 - val_loss: 0.1170 - val_mse: 6.4295e-04\n",
            "Epoch 47/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.3284e-04\n",
            "Epoch 47: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.1148 - mse: 1.3284e-04 - val_loss: 0.1170 - val_mse: 6.5037e-04\n",
            "Epoch 48/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.3353e-04\n",
            "Epoch 48: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 36s 29ms/step - loss: 0.1148 - mse: 1.3352e-04 - val_loss: 0.1170 - val_mse: 6.4115e-04\n",
            "Epoch 49/50\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.1148 - mse: 1.3139e-04\n",
            "Epoch 49: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.3139e-04 - val_loss: 0.1171 - val_mse: 6.6161e-04\n",
            "Epoch 50/50\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1148 - mse: 1.3137e-04\n",
            "Epoch 50: loss did not improve from 0.11493\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.1148 - mse: 1.3135e-04 - val_loss: 0.1170 - val_mse: 6.4438e-04\n"
          ]
        }
      ],
      "source": [
        "filepath=\"/content/gdrive/My Drive/my_model\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, \n",
        "                                                monitor='loss', verbose=1, \n",
        "                                                save_best_only=True,\n",
        "                                                mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "results = model.fit(x_train, \n",
        "                    y_train, \n",
        "                    epochs=50, \n",
        "                    batch_size=64,\n",
        "                    use_multiprocessing=True,\n",
        "                    callbacks = callbacks_list,\n",
        "                    steps_per_epoch=x_train.shape[0]/batchsize, \n",
        "                    validation_data=(x_val, y_val),\n",
        "                    validation_batch_size=batchsize,\n",
        "                    validation_steps=(x_val.shape[0]/batchsize))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "tfEiggpQ1Doz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5f0zMC1XOnl",
        "outputId": "1790eefe-96c4-4d23-b7a1-5af9fd72c73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1177 - mse: 6.5242e-04\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1177 - mse: 6.4639e-04\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1173 - mse: 6.5657e-04\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1175 - mse: 6.5366e-04\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1176 - mse: 6.6087e-04\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1177 - mse: 6.5030e-04\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1172 - mse: 6.5084e-04\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1179 - mse: 6.5125e-04\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1176 - mse: 6.5245e-04\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1176 - mse: 6.4932e-04\n",
            "3.80261587260337e-06\n"
          ]
        }
      ],
      "source": [
        "MSEs = []\n",
        "for i in range(10):\n",
        "  x_test, y_test = next(test_generator)\n",
        "  mse = model.evaluate(x_test, y_test)[1]\n",
        "  MSEs.append(mse)\n",
        "  \n",
        "print(np.std(MSEs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see some output results."
      ],
      "metadata": {
        "id": "Ke3QnuAl_4o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test = next(test_generator)\n",
        "t = x_test[0].reshape(1,32,32,1)\n",
        "y_t = y_test[0]\n",
        "\n",
        "res = model.predict(t, \n",
        "                    batch_size=batchsize, \n",
        "                    verbose = 1,\n",
        "                    steps = 32)\n",
        "\n",
        "plt.imshow(x_test[0],cmap='gray', interpolation='nearest')\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(np.squeeze(res[0]), cmap='gray', interpolation='nearest')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "4hMN8oIXtUzQ",
        "outputId": "bed9d173-6e88-4ee5-c794-736bcac3a8be"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 1/32 [..............................] - ETA: 0sWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r32/32 [==============================] - 0s 748us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i saved the model on my drive."
      ],
      "metadata": {
        "id": "dZOAwFJT__9l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6ZJulqdR8a5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab79e9e7-a385-43b5-ae11-a29590cc0691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#model.save('./gdrive/MyDrive/my_Newmodel')\n",
        "model = tf.keras.models.load_model('./gdrive/MyDrive/my_Newmodel')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('./gdrive/MyDrive/lastWeights3.h5')"
      ],
      "metadata": {
        "id": "jdeHdlI2ui2j"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('./gdrive/MyDrive/lastWeights2.h5')"
      ],
      "metadata": {
        "id": "I34hWX98uiYR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Gabriele_Risino.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}